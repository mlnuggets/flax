{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYWmaWc0Lx1B"
   },
   "source": [
    "# Elegy(High-level API for deep learning in JAX & Flax)\n",
    "\n",
    "Click the image below to read the post online.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://www.machinelearningnuggets.com/elegy-flax-jax\"><img src=\"https://digitalpress.fra1.cdn.digitaloceanspaces.com/mhujhsj/2022/07/logho-1.png\" alt=\"Open in ML Nuggets\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RXGNpXONkZQ"
   },
   "outputs": [],
   "source": [
    "pip install -U elegy flax jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxbhMx0v1GZB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Obtain from https://www.kaggle.com/username/account\n",
    "os.environ[\"KAGGLE_USERNAME\"]=\"KAGGLE_USERNAME\"\n",
    "os.environ[\"KAGGLE_KEY\"]=\"KAGGLE_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnevvxZHL3qB"
   },
   "outputs": [],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HnlMN4nL3sU",
    "outputId": "d4a6d819-d7e0-46de-cd6d-91bcdeeeb66c"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "641Fk3GPL3up"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('imdb-dataset-of-50k-movie-reviews.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('imdb-dataset-of-50k-movie-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB2h-dFFL3wr"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzGvQEzDMA_T"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmN9ruPqMA9B",
    "outputId": "9ab2c7d7-53ce-4a23-88e4-15c2e2cacd58"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "def remove_stop_words(review):\n",
    "    review_minus_sw = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    review = review.split()\n",
    "    cleaned_review = [review_minus_sw.append(word) for word in review if word not in stop_words]            \n",
    "    cleaned_review = ' '.join(review_minus_sw)\n",
    "    return cleaned_review       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWZ1Q5WNMA4g"
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_stop_words)\n",
    "labelencoder = LabelEncoder()\n",
    "df = df.assign(sentiment = labelencoder.fit_transform(df[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBxCCBuZMH90"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = df.drop_duplicates()\n",
    "docs = df['review']\n",
    "labels = array(df['sentiment'])\n",
    "X_train, X_test , y_train, y_test = train_test_split(docs, labels , test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v677KDxAMIAp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "max_features = 10000  # Maximum vocab size.\n",
    "batch_size = 128\n",
    "max_len = 50 # Sequence length to pad the outputs to.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(standardize='lower_and_strip_punctuation',max_tokens=max_features,output_mode='int',output_sequence_length=max_len)\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CTTajMyMICX"
   },
   "outputs": [],
   "source": [
    "X_train_padded =  vectorize_layer(X_train)\n",
    "X_test_padded =  vectorize_layer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJqWE6NNMO_2"
   },
   "outputs": [],
   "source": [
    "training_data = tf.data.Dataset.from_tensor_slices((X_train_padded, y_train))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((X_test_padded, y_test))\n",
    "training_data = training_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79dPvG9PMPCX"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "def get_train_batches():\n",
    "  ds = training_data.prefetch(1)\n",
    "  ds = ds.repeat(3)\n",
    "  ds = ds.shuffle(3, reshuffle_each_iteration=True)\n",
    "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "  return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q1DXf14MPEd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCsNSpdeMPGj"
   },
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def setup(self):\n",
    "        self.embedding = nn.Embed(max_features, max_len)\n",
    "        lstm_layer = nn.scan(nn.OptimizedLSTMCell,\n",
    "                               variable_broadcast=\"params\",\n",
    "                               split_rngs={\"params\": False},\n",
    "                               in_axes=1, \n",
    "                               out_axes=1,\n",
    "                               length=max_len,\n",
    "                               reverse=False)\n",
    "        self.lstm1 = lstm_layer()\n",
    "        self.dense1 = nn.Dense(256)\n",
    "        self.lstm2 = lstm_layer()\n",
    "        self.dense2 = nn.Dense(128)\n",
    "        self.lstm3 = lstm_layer()\n",
    "        self.dense3 = nn.Dense(64)\n",
    "        self.dense4 = nn.Dense(2)\n",
    "        \n",
    "    @nn.remat    \n",
    "    def __call__(self, x_batch):\n",
    "        x = self.embedding(x_batch)\n",
    "        \n",
    "        carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=128)\n",
    "        (carry, hidden), x = self.lstm1((carry, hidden), x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=64)\n",
    "        (carry, hidden), x = self.lstm2((carry, hidden), x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = nn.relu(x)\n",
    "        \n",
    "        carry, hidden = nn.OptimizedLSTMCell.initialize_carry(jax.random.PRNGKey(0), batch_dims=(len(x_batch),), size=32)\n",
    "        (carry, hidden), x = self.lstm3((carry, hidden), x)\n",
    "        \n",
    "       \n",
    "        x = self.dense3(x)\n",
    "        x = nn.relu(x)\n",
    "        x = self.dense4(x[:, -1])\n",
    "        return nn.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K993A1K11GZH"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import elegy as eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM8PdElf1GZL",
    "outputId": "e0690cf6-09ac-4efc-8584-d77238e6cad0"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "model = eg.Model(\n",
    "    module=LSTMModel(),\n",
    "    loss=[\n",
    "        eg.losses.Crossentropy(),\n",
    "        eg.regularizers.L2(l=1e-4),\n",
    "    ],\n",
    "    metrics=eg.metrics.Accuracy(),\n",
    "    optimizer=optax.adam(1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeFqaBWpLx1H",
    "outputId": "ef2ba658-40e8-4cba-da8e-55dc074d2d6b"
   },
   "outputs": [],
   "source": [
    "model.summary(jnp.array(X_train_padded[:64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYrf93XbOsqJ"
   },
   "outputs": [],
   "source": [
    "model = model.distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfRIJ1uG1GZW",
    "outputId": "e55fb685-219c-4b26-94a7-e071d66a0aa7"
   },
   "outputs": [],
   "source": [
    "callbacks = [ eg.callbacks.TensorBoard(\"logs\"),\n",
    "             eg.callbacks.ModelCheckpoint(\"models/high-level\", save_best_only=True),\n",
    "             eg.callbacks.EarlyStopping(monitor = 'val_loss',patience=5)\n",
    "            ]\n",
    "history = model.fit(\n",
    "    training_data,\n",
    "    epochs=100,\n",
    "    validation_data=(validation_data),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAfc-JNd1GZb",
    "outputId": "946e6a69-1271-4f91-890c-d790db183bc6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "    n_plots = len(history.history.keys()) // 2\n",
    "    plt.figure(figsize=(14, 24))\n",
    "    \n",
    "    for i, key in enumerate(list(history.history.keys())[:n_plots]):\n",
    "        metric = history.history[key]\n",
    "        val_metric = history.history[f\"val_{key}\"]\n",
    "\n",
    "        plt.subplot(n_plots, 1, i + 1)\n",
    "        plt.plot(metric, label=f\"Training {key}\")\n",
    "        plt.plot(val_metric, label=f\"Validation {key}\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.ylabel(key)\n",
    "        plt.title(f\"Training and Validation {key}\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCNlJ74S1GZf"
   },
   "outputs": [],
   "source": [
    "(text, test_labels) = next(iter(validation_data))\n",
    "\n",
    "y_pred = model.predict(jnp.array(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lak2-b9m1GZw",
    "outputId": "4684c49e-5f57-4d36-daaf-777672a98fcf"
   },
   "outputs": [],
   "source": [
    "# You can use can use `save` but `ModelCheckpoint already serialized the model\n",
    "# model.save(\"model\")\n",
    "\n",
    "# current model reference\n",
    "print(\"current model id:\", id(model))\n",
    "\n",
    "# load model from disk\n",
    "model = eg.load(\"models/high-level\")\n",
    "\n",
    "# new model reference\n",
    "print(\"new model id:    \", id(model))\n",
    "\n",
    "# check that it works!\n",
    "model.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjvGBgU31GZz"
   },
   "source": [
    "## Where to go from here\n",
    "Follow us on [LinkedIn](https://www.linkedin.com/company/mlnuggets), [Twitter](https://twitter.com/ml_nuggets), [GitHub](https://github.com/mlnuggets) and subscribe to our [blog](https://www.machinelearningnuggets.com/#/portal) so that you don't miss a new issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
